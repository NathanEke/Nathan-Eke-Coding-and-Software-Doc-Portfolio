{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6f55b2f",
   "metadata": {},
   "source": [
    "# Final Project - Part 1\n",
    "\n",
    "<font size=\"4\" color=\"red\">100/100pts</font>\n",
    "\n",
    "### Your Name: Nathan Eke\n",
    "\n",
    "### 1. Write a brief description of the texts you used for Part 1.\n",
    "For part 1, I compared Steam descriptions of First-Person Shooter (FPS) games. One group is composed of 10 FPS games with a competitive, player vs. player focus (CFPS). The other group is 10 FPS games that are all non-competitive or focused on single-player experiences (NCFPS). \n",
    "\n",
    "### 2. A brief Summary of your findings\n",
    "Given that the medium of Steam descriptions requires brief paragraphs, the average amount of words in both sets of files was around 32-33. TTR and average word length were also quite similar, at around .65 and 5.2-5.5 respectively. Neither folder of games has much room to repeat themselves or use particularly long words. Both groups understandably tended to use the term \"shooter\" somewhere in their description. \n",
    "\n",
    "I found that the CFPS games tended to announce the fact that they were free to play in their descriptions. Also, common words like \"tactical,\" \"team-based,\" and \"arena\" emphasize the competitive nature of the genre. Meanwhile, many of the NCFPS games used the word \"world,\" likely to draw attention to the larger game worlds you play in compared to competitive FPS games. \"Enemies\" pops up in common words as well, which I think signals how NCFPS games don't put you against real players. And I think that lack of player vs. player gameplay also explains another common word, \"unleash,\" as NCFPS games can be more of a power fantasy where players can let loose on computer controlled enemies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c4025",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "### Before you start writing your code\n",
    "-- **Review the assignment description on Canvas carefully**\n",
    "- This notebook provides you with a template program for your medterm project based on the functions included in Unit 6 of the textbook. **Make sure that you have a good understanding of the code introduced in Unit 6.**\n",
    "- Create a corpus according to the instructions givenon Canvas. Make sure to create your corpus in the folder where this notbook is saved.\n",
    "\n",
    "\n",
    "### Step 1. Write a program that computes the following text analysis metrics, plus one additional text-related measure of your choice, for each document:\n",
    "\n",
    "- An average word length (i.e., number of characters in a word)\n",
    "- The top 10 most frequently used words (excluding function words)\n",
    "- The top 10 most frequently used bigrams, and trigrams (bigrams and trigrams should include function words)\n",
    "\n",
    "This program consists of multiple functions. The main function is `analyze_a_file` defined below. This function calls 6 functions that calculate the followin gitems.\n",
    "\n",
    "First, you should write functions to calculate the items listed above. **Use the code cells provided below for these functions.** For the 3rd item, you will need to write 2 functions, for bigrams and trigrams.\n",
    "\n",
    "After you have written and tested these functions individually, add these functions to `analyze_a_file`.\n",
    "\n",
    "### Step 2.  Using the function you wrote for Step 1, write a function that computes the following items for each group\n",
    "\n",
    "- An average word length\n",
    "- The top 10 most frequently used words (excluding function words)\n",
    "- The top 10 most frequently used bigrams and trigrams. (bigrams and trigrams should include function words)\n",
    "\n",
    "This program also consists of multiple functions, and the main function is `analyze_a_corpus` defined below. Study `analyze_a_corpus` and `save_results` carefully first. Then, modify these functions to complete the assignment.\n",
    "\n",
    "#### NOTE: Your functions for identifying the top 10 most words/bigrams/trigrams will likely return a list. This means that you will need to conver it into a string before you can write it to a CSV file.\n",
    "\n",
    "Make sure that:\n",
    "\n",
    "- the spreadsheet does not include counts.\n",
    "- the spreadsheet does not include brackets and parentheses (e.g., \"(\", \")\", \"[\", and \"]\").\n",
    "\n",
    "**Use the `lst_to_str` function provided below to convert a list of words or a list of tuples (for bigrams/trigrams) into a string** list_to_str.ipynb in this folder includes some sample usages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f05683",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "203ab45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', 'and', 'of', 'having', 'nothing', 'to', 'do:', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', '`and', 'what', 'is', 'the', 'use', 'of', 'a', \"book,'\", 'thought', 'Alice', '`without', 'pictures', 'or', \"conversation?'\"]\n"
     ]
    }
   ],
   "source": [
    "# Lab 4.2.8\n",
    "def make_word_list(fpath):\n",
    "    \"\"\"\n",
    "    This function takes a fpath as its argument, read the text from the file,\n",
    "    split (tokenize) the text into a list of words, and return the list.\n",
    "    \"\"\"\n",
    "    with open(fpath, errors=\"ignore\") as fin: # Open 'file_path', and create a file handler: 'fin'\n",
    "        text  = fin.read()                    # Read the entire content of the file.\n",
    "        words = text.split()                  # Create a list of words.\n",
    "\n",
    "        result = list()\n",
    "        for w in words:\n",
    "            if w[-1] == '.' or w[-1] == ',' or w[-1] == ';':\n",
    "                result.append(w[:-1])\n",
    "            else:\n",
    "                result.append(w)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Test\n",
    "words = make_word_list(\"test/corpus/alices_adventures_in_wonderland_p1.txt\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25930768",
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Look at 'list_to_str.ipynb' included in this file for more details about this function.\n",
    "## You will be using this function with get_frequent_words, get_bigrams, ahnd get_trigrams.\n",
    "\n",
    "def list_to_str(lst):\n",
    "    \"\"\"\n",
    "    This function takes a list of words or a list of tuples as its argument, and \n",
    "    returns a list of words or ngrams (tuples) to a string that \n",
    "    can be written to a single cell in a CSV file.\n",
    "    \"\"\"\n",
    "    s = ''\n",
    "    for i in range(len(lst)):  # for each tuple\n",
    "\n",
    "        # Create a string from a tuple (e.g., \"the united nations\")\n",
    "        if type(lst[i]) == tuple:\n",
    "            s += ' '.join(lst[i])\n",
    "        else:\n",
    "            s += lst[i]\n",
    "\n",
    "        # Insert a '|' character if i is not the last index.\n",
    "        # (e.g., \"the united nations | i believe that\")\n",
    "        if i < len(lst)-1:           \n",
    "            s += ' | '\n",
    "            \n",
    "    return \"{}\".format(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec77a5c",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Function for computing text analysis metrics\n",
    "\n",
    "The following code cells include functions for computing the following text analysis metrics:\n",
    "\n",
    "- A type-token ratio (The total # of types divided by the total # of tokens).\n",
    "- The total number of words\n",
    "- An average word length (i.e., number of characters in a word)*\n",
    "- The top 10 most frequently used words (excluding function words)*\n",
    "- The top 10 most frequently used bigrams (bigrams should include function words)*\n",
    "- The top 10 most frequently used trigrams (trigrams should include function words)*\n",
    "\n",
    "\n",
    "*You will need to write these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e263cbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "def count_words(list_of_words):\n",
    "    \"\"\"\n",
    "    This function takes a list of strings as its argument, and returns the total\n",
    "    number of strings in the list.\n",
    "    \"\"\"\n",
    "    count = len(list_of_words)\n",
    "    return count\n",
    "\n",
    "# Test\n",
    "words = make_word_list(\"test/corpus/alices_adventures_in_wonderland_p1.txt\")\n",
    "count = count_words(words)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b915daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "---\n",
      "0.7543859649122807\n"
     ]
    }
   ],
   "source": [
    "def get_ttr(list_of_words):\n",
    "    \"\"\"\n",
    "    This function takes a tokenized text (list of words) as its argument, and returns\n",
    "    the type token ratio of the text.\n",
    "    \"\"\"\n",
    "    unique_words = []\n",
    "    \n",
    "    # Create a list of unique words\n",
    "    for word in list_of_words:\n",
    "        if word not in unique_words:            # If 'word' is not in 'unique_words',\n",
    "            unique_words.append(word)           # Append 'word' to 'unique_words'.\n",
    "            \n",
    "    # Calculate the TTR (divide the total # of unique words by the total # of words)    \n",
    "    ttr = len(unique_words)/len(list_of_words)\n",
    "    return ttr\n",
    "\n",
    "# Simple Test\n",
    "ttr = get_ttr([\"dog\", \"dog\", \"cat\"])  # 2 types, 3 tokens. ttr = 2/3\n",
    "print(ttr)\n",
    "print(\"---\")\n",
    "\n",
    "# Test\n",
    "words = make_word_list(\"test/corpus/alices_adventures_in_wonderland_p1.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(ttr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c62118aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.245614035087719\n"
     ]
    }
   ],
   "source": [
    "# An average word length in a file (i.e., average number of characters in a word)\n",
    "def make_word_list(fpath):\n",
    "    \"\"\"\n",
    "    This function takes a fpath as its argument, read the text from the file,\n",
    "    split (tokenize) the text into a list of words, and return the list.\n",
    "    \"\"\"\n",
    "    with open(fpath, errors=\"ignore\") as fin: # Open 'file_path', and create a file handler: 'fin'\n",
    "        text  = fin.read()                    # Read the entire content of the file.\n",
    "        words = text.split()                  # Create a list of words.\n",
    "\n",
    "        result = list()\n",
    "        for w in words:\n",
    "            if w[-1] == '.' or w[-1] == ',' or w[-1] == ';':\n",
    "                result.append(w[:-1])\n",
    "            else:\n",
    "                result.append(w)\n",
    "\n",
    "    return result\n",
    "def count_words(list_of_words):\n",
    "    \"\"\"\n",
    "    This function takes a list of strings as its argument, and returns the total\n",
    "    number of strings in the list.\n",
    "    \"\"\"\n",
    "    count = len(list_of_words)\n",
    "    return count\n",
    "def get_average_word_length(list_of_words):\n",
    "    \"\"\"\n",
    "    This function takes a list of words as its argument, and returns the averge length \n",
    "    of all the words in the list\n",
    "    \"\"\"\n",
    "\n",
    "    countw = count_words(list_of_words)\n",
    "    countc = 0 # variable will become total number of characters in a list\n",
    "    for word in list_of_words: # for each word in list_of_words, get the length of the word and add it to countc.\n",
    "        length = len(word)\n",
    "        countc+=length\n",
    "    avg = countc/countw # divice the total number of characters in the list by the number of words in the list.\n",
    "    return avg\n",
    "\n",
    "# Test\n",
    "words = make_word_list(\"test/corpus/alices_adventures_in_wonderland_p1.txt\")\n",
    "average = get_average_word_length(words)\n",
    "print(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5516ce75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sister | pictures | alice | use | twice | tired | thought | sitting | reading | peeped\n"
     ]
    }
   ],
   "source": [
    "# The top 10 most frequently used words (excluding function words)\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "english_stops = stopwords.words('english') \n",
    "def get_frequent_words(list_of_words):\n",
    "    \"\"\"\n",
    "    This function takes a list of words as its argument, and returns the top 10 most\n",
    "    frequently used words.\n",
    "    \"\"\"\n",
    "    dict={}\n",
    "    for item in list_of_words: # get the frequency of each word in list_of_words and add it to dict\n",
    "        item = item.lower()\n",
    "        last = item[-1]\n",
    "        if last in string.punctuation:\n",
    "            item=item[:-1]\n",
    "        \n",
    "        if item not in dict and item not in english_stops:\n",
    "            dict[item] = 1\n",
    "        elif item in dict:\n",
    "            dict[item]+=1\n",
    "\n",
    "    lst=[]\n",
    "    for key, value in dict.items(): # add each value, key pair in dict to lst\n",
    "        t = (value, key)        \n",
    "        lst.append(t)\n",
    "    lst.sort(reverse=True)  # sort list from highest value to lowest value\n",
    "    \n",
    "    new=[]\n",
    "    num=0\n",
    "    for count, word in lst: # add the first 10 words in lst to new \n",
    "        if num<=9:\n",
    "            new.append((word))\n",
    "            num+=1\n",
    "    new = list_to_str(new) # turn new into a string\n",
    "    return new\n",
    "\n",
    "# Test\n",
    "words = make_word_list(\"test/corpus/alices_adventures_in_wonderland_p1.txt\")\n",
    "frequent_words = get_frequent_words(words)\n",
    "print(frequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fe5417a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of the | he was | was a | that he | years without | world people | without growing | whom little | who might | which sheridan\n"
     ]
    }
   ],
   "source": [
    "# The top 10 most frequently used bigrams, and trigrams (bigrams and trigrams should include function words)\n",
    "from nltk.tokenize import RegexpTokenizer      \n",
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "puncts = string.punctuation\n",
    "def strip_punct(s):\n",
    "    res = \"\"                       \n",
    "    for c in s:                    \n",
    "        if c in puncts:            \n",
    "            continue               \n",
    "        else:                      \n",
    "            res = res + c\n",
    "    return res  \n",
    "\n",
    "def get_bigrams(list_of_words):\n",
    "    \"\"\"\n",
    "    This function takes a list of words as its argument, and returns the top 10 most\n",
    "    frequently used bigrams.\n",
    "    \"\"\"\n",
    "    d = dict()\n",
    "    last_index = len(list_of_words)-1 # Gets the total number of words in list_of_words minus 1\n",
    "    i = 0 # Assigns i the value of 0\n",
    "    while i < last_index: # While i is less than the last index of list_of_words\n",
    "\n",
    "        w = list_of_words[i] # Assigns w the value of words[i]\n",
    "        \n",
    "        clean_w = strip_punct(w) # Strip the punctuation from a word\n",
    "        clean_w = clean_w.lower() # Converts all letters in a word to lowercase\n",
    "            \n",
    "        next_w = (list_of_words[i+1]) # Defines next_w as the word immeditaly after words[i] and strips the punctuations \n",
    "        next_w = strip_punct(next_w)\n",
    "        next_w = next_w.lower() # Converts next_w to lowercase\n",
    "\n",
    "        t = (clean_w, next_w) # Defines t as every duo of words\n",
    "\n",
    "        if t not in d: # if t is not d...\n",
    "            d[t] = 1 # t is added to the dicitonary with a value of 1\n",
    "        else: # Otherwise...\n",
    "            d[t] = d[t] + 1 # The value of t in the dictionary is increased by 1\n",
    "            \n",
    "        i = i + 1 # Increase i by 1\n",
    "        \n",
    "    # sort\n",
    "    lst = list() # Initialzes lst as an empty list\n",
    "    for key, val in d.items(): # For each key, value pair in d...\n",
    "         lst.append((val, key)) # Add value, key to the lst\n",
    "        \n",
    "    lst.sort(reverse=True) # Sort the list from highest to lowest value\n",
    "    results = []\n",
    "    for key, val in lst[:10]: # add the first 10 values in lst to results\n",
    "        results.append(val)\n",
    "    final = list_to_str(results) # convert results into a string\n",
    "    return final \n",
    "\n",
    "# Test\n",
    "words = make_word_list(\"test/corpus/around_the_world_in_80_days_p1.txt\")\n",
    "bigrams = get_bigrams(words)\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f824ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without pictures or | what is the | was reading but | was beginning to | very tired of | use of a | twice she had | to get very | to do once | tired of sitting\n"
     ]
    }
   ],
   "source": [
    "# The top 10 most frequently used bigrams, and trigrams (bigrams and trigrams should include function words)\n",
    "from nltk.tokenize import RegexpTokenizer      \n",
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "puncts = string.punctuation\n",
    "\n",
    "def strip_punct(s): #strips punctuation from a word\n",
    "    res = \"\"                       \n",
    "    for c in s:                    \n",
    "        if c in puncts:            \n",
    "            continue               \n",
    "        else:                      \n",
    "            res = res + c\n",
    "    return res  \n",
    "\n",
    "def get_trigrams(list_of_words):\n",
    "    \"\"\"\n",
    "    This function takes a list of words as its argument, and returns the top 10 most\n",
    "    frequently used trigrams.\n",
    "    \"\"\"\n",
    "    d = dict()\n",
    "    last_index = len(list_of_words) -1 # Gets the total number of words in words minus 1\n",
    "    i = 0 # Assigns i the value of 0\n",
    "    while i < last_index: # While i is less than the last index of words\n",
    "\n",
    "        w = list_of_words[i] # Assigns w the value of words[i]\n",
    "        \n",
    "        clean_w = strip_punct(w) # Strip the punctuation from a word\n",
    "        clean_w = clean_w.lower() # Converts all letters in a word to lowercase\n",
    "            \n",
    "        prev_w = strip_punct(list_of_words[i-1]) # Defines prev_w as the word immeditaly before words[i] and strips the punctuations \n",
    "        prev_w = prev_w.lower() # Converts prev_w to lowercase\n",
    "        next_w = strip_punct(list_of_words[i+1]) # Defines next_w as the word immeditaly after words[i] and strips the punctuations \n",
    "        next_w = next_w.lower() # Converts next_w to lowercase\n",
    "\n",
    "        t = (prev_w, clean_w, next_w) # Defines t as every trio of words where the middle word (clean_w) is not a fucntion word\n",
    "\n",
    "        if t not in d: # if t is not d...\n",
    "            d[t] = 1 # t is added to the dicitonary with a value of 1\n",
    "        else: # Otherwise...\n",
    "            d[t] = d[t] + 1 # The value of t in the dictionary is increased by 1\n",
    "            \n",
    "        i = i + 1 # Increase i by 1\n",
    "        \n",
    "    # sort\n",
    "    lst = list() # Initialzes lst as an empty list\n",
    "    for key, val in d.items(): # For each key, value pair in d...\n",
    "         lst.append((val, key)) # Add value, key to the lst\n",
    "        \n",
    "    lst.sort(reverse=True) # Sort the list from highest to lowest value\n",
    "    results = []\n",
    "    for key, val in lst[:10]: # add the first 10 values in lst to results\n",
    "        results.append(val)\n",
    "    final = list_to_str(results) # convert results into a string\n",
    "    return final # Return the first n items in the list\n",
    "\n",
    "# Test\n",
    "words = make_word_list(\"test/corpus/alices_adventures_in_wonderland_p1.txt\")\n",
    "trigrams = get_trigrams(words)\n",
    "print(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51367a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN | NNP | VBD | DT | NN | JJ | PRP | CD | WP | VBG\n"
     ]
    }
   ],
   "source": [
    "# Write a function for computing a text-related measure of your choice here.\n",
    "# Make sure to include a code to test your function.\n",
    "import nltk\n",
    "    \n",
    "def get_pos(list_of_words):\n",
    "    \"\"\"\n",
    "    This function takes a list of words as its argument, and returns the top 10 most\n",
    "    frequentl pos tags.\n",
    "    \"\"\"\n",
    "    pos_dict = nltk.pos_tag(list_of_words)\n",
    "    tags = {}\n",
    "    for key, value in pos_dict: # Fill tags dictionary with all pos tags in list_of_words and how many times each occurs.\n",
    "        if value not in tags:\n",
    "            tags[value] = 1\n",
    "        else:\n",
    "            tags[value] = tags[value] + 1\n",
    "    tag_lst = list(tags.items()) # turns tags dictionary into a list\n",
    "    \n",
    "    sortedlst = []\n",
    "    for key, val in tag_lst:          # For each key-value pair in 'tag_lst',\n",
    "        t = (val, key)            # Create a tuple ('val', 'key')\n",
    "        sortedlst.append(t)   \n",
    "    sortedlst.sort(reverse=True)\n",
    "    \n",
    "    top10pos=[]\n",
    "    num=0\n",
    "    for count, word in sortedlst: # add the first 10 words in sortedlst to new \n",
    "        if num<=9:\n",
    "            top10pos.append((word))\n",
    "            num+=1\n",
    "            \n",
    "    top10pos_str = list_to_str(top10pos) # turn top10pos into a string\n",
    "    return top10pos_str\n",
    "    \n",
    "# Test\n",
    "words = make_word_list(\"test/corpus/around_the_world_in_80_days_p1.txt\")\n",
    "pos = get_pos(words)\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f1a57b",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## analyze_a_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46b8ef8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'big': 'of the | he was | was a | that he | years without | world people | '\n",
      "           'without growing | whom little | who might | which sheridan',\n",
      "    'filename': 'around_the_world_in_80_days_p1.txt',\n",
      "    'gawl': 4.433333333333334,\n",
      "    'gfw': 'byron | years | world | without | tranquil | thousand | though | '\n",
      "           'sheridan | seemed | saville',\n",
      "    'num_words': 90,\n",
      "    'pos': 'IN | NNP | VBD | DT | NN | JJ | PRP | CD | WP | VBG',\n",
      "    'trig': 'he was a | years without growing | world people said | without '\n",
      "            'growing old | whom little was | who might live | which sheridan '\n",
      "            'died | was one of | was known except | was byronic but',\n",
      "    'ttr': 0.7777777777777778}\n",
      "---\n",
      "['Mr', 'Phileas', 'Fogg', 'lived', 'in', '1872', 'at', 'No', '7', 'Saville', 'Row', 'Burlington', 'Gardens', 'the', 'house', 'in', 'which', 'Sheridan', 'died', 'in', '1814', 'He', 'was', 'one', 'of', 'the', 'most', 'noticeable', 'members', 'of', 'the', 'Reform', 'Club', 'though', 'he', 'seemed', 'always', 'to', 'avoid', 'attracting', 'attention', 'an', 'enigmatical', 'personage', 'about', 'whom', 'little', 'was', 'known', 'except', 'that', 'he', 'was', 'a', 'polished', 'man', 'of', 'the', 'world', 'People', 'said', 'that', 'he', 'resembled', 'Byron', '-', 'at', 'least', 'that', 'his', 'head', 'was', 'Byronic', 'but', 'he', 'was', 'a', 'bearded', 'tranquil', 'Byron', 'who', 'might', 'live', 'on', 'a', 'thousand', 'years', 'without', 'growing', 'old']\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import pprint                              \n",
    "pp = pprint.PrettyPrinter(indent=4)  \n",
    "\n",
    "def get_ttr(list_of_words):\n",
    "    \"\"\"\n",
    "    This function takes a tokenized text (list of words) as its argument, and returns\n",
    "    the type token ratio of the text.\n",
    "    \"\"\"\n",
    "    unique_words = []\n",
    "    \n",
    "    # Create a list of unique words\n",
    "    for word in list_of_words:\n",
    "        if word not in unique_words:            # If 'word' is not in 'unique_words',\n",
    "            unique_words.append(word)           # Append 'word' to 'unique_words'.\n",
    "            \n",
    "    # Calculate the TTR (divide the total # of unique words by the total # of words)    \n",
    "    ttr = len(unique_words)/len(list_of_words)\n",
    "    return ttr\n",
    "    \n",
    "def count_words(list_of_words):\n",
    "    \"\"\"\n",
    "    This function takes a list of strings as its argument, and returns the total\n",
    "    number of strings in the list.\n",
    "    \"\"\"\n",
    "    count = len(list_of_words)\n",
    "    return count\n",
    "    \n",
    "def make_word_list(fpath):\n",
    "    \"\"\"\n",
    "    This function takes a fpath as its argument, read the text from the file,\n",
    "    split (tokenize) the text into a list of words, and return the list.\n",
    "    \"\"\"\n",
    "    with open(fpath, errors=\"ignore\") as fin: # Open 'file_path', and create a file handler: 'fin'\n",
    "        text  = fin.read()                    # Read the entire content of the file.\n",
    "        words = text.split()                  # Create a list of words.\n",
    "\n",
    "        result = list()\n",
    "        for w in words:\n",
    "            if w[-1] == '.' or w[-1] == ',' or w[-1] == ';':\n",
    "                result.append(w[:-1])\n",
    "            else:\n",
    "                result.append(w)\n",
    "    return result\n",
    "    \n",
    "def analyze_a_file(in_file_path):\n",
    "    \"\"\"\n",
    "    This function takes a file name path as an argument, and analyzes the content\n",
    "    of the file, and returns (1) the results in a dictionary and (2) the list of words\n",
    "    in the file.\n",
    "    \"\"\"\n",
    "    fpath = pathlib.Path(in_file_path)             # Create a filepath object for 'in_file_path'\n",
    "    list_of_words = make_word_list(in_file_path)   # Create a list of words in the file.\n",
    "    num_words = count_words(list_of_words)         # Count the total # of words in the file.\n",
    "    ttr = get_ttr(list_of_words)                   # Get ttr of a file\n",
    "    gawl = get_average_word_length(list_of_words)  # Get average word length of file\n",
    "    gfw = get_frequent_words(list_of_words)        # Get the top 10 most frequent words in a file\n",
    "    big = get_bigrams(list_of_words)               # Get the top 10 most frequent bigrams in a file\n",
    "    trig = get_trigrams(list_of_words)             # Get the top 10 most frequent trigrams in a file\n",
    "    conc = get_pos(list_of_words)                  # Get the top 10 most frequent pos tags in a file\n",
    "    \n",
    "    results = dict()\n",
    "    results['filename'] = fpath.name\n",
    "    results['num_words'] = num_words\n",
    "    results['ttr'] = ttr\n",
    "    results['gawl'] = gawl\n",
    "    results['gfw'] = gfw\n",
    "    results['big'] = big\n",
    "    results['trig'] = trig\n",
    "    results['pos'] = get_pos(list_of_words)\n",
    "    \n",
    "    # NEW: Notice that this function returns 2 items.\n",
    "    return results, list_of_words\n",
    "\n",
    "# Test\n",
    "res, words = analyze_a_file(\"test/corpus/around_the_world_in_80_days_p1.txt\")\n",
    "pp.pprint(res)\n",
    "print(\"---\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedd923e",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## save_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55926427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check test.csv in Excel or Numbers (Mac OS)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pprint                              \n",
    "pp = pprint.PrettyPrinter(indent=4)  \n",
    "\n",
    "def save_results(list_of_results, corpus_results, csv_fpath):\n",
    "    \"\"\"\n",
    "    This function saves the results of a text analysis into a CSV file. You may extend\n",
    "    this function to write more results to the file.\n",
    "    \"\"\"\n",
    "    is_header = True\n",
    "    with open(csv_fpath, 'w') as fout:         # Open 'csv_path'.\n",
    "        writer = csv.writer(fout)              # Create a csv writer object.\n",
    "        \n",
    "        for results in list_of_results:        # For each set of results in 'list_of_results'.\n",
    "            if is_header:\n",
    "                header_row = results.keys()    # Create a list of header items.\n",
    "                writer.writerow(header_row)    # Write the hader items.            \n",
    "                is_header = False\n",
    "            row = results.values()\n",
    "            writer.writerow(row)               # Write the values.\n",
    "\n",
    "        row = corpus_results.values()          # NEW - Add a row for the group (corpus_results)\n",
    "        writer.writerow(row)\n",
    "\n",
    "# Test\n",
    "# Create a simple list of results for testing\n",
    "res1, words = analyze_a_file(\"test/corpus/alices_adventures_in_wonderland_p1.txt\")\n",
    "res2, words = analyze_a_file(\"test/corpus/around_the_world_in_80_days_p1.txt\")\n",
    "res_lst = [res1, res2]         # make a simple list of results with 2 dictionaries\n",
    "\n",
    "# Create a simple corpus result data for testing\n",
    "corpus_results = dict()\n",
    "corpus_results['folder'] = \"test_folder\"\n",
    "corpus_results['num_words'] = 123\n",
    "corpus_results['ttr'] = 0.5\n",
    "\n",
    "save_results(res_lst, corpus_results, \"test/results.csv\")\n",
    "print(\"Check test.csv in Excel or Numbers (Mac OS)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b49372",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### anayze_a_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d410c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results are stored in test/results.csv\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "def analyze_a_corpus(folder, csv_fpath):\n",
    "    \"\"\"\n",
    "    This function takes a folder as an argument, and analyzes all the text files \n",
    "    in the folder, then writes the results in a CSV file.\n",
    "    \"\"\"\n",
    "    list_of_words = list()                     # Initialize a list that holds all the words in the corpus\n",
    "    list_of_results = list()                   # Initialize a list that holds all the results\n",
    "    file_count = 0\n",
    "    \n",
    "    folder_path = pathlib.Path(folder)         # Make a path object for the folder 'folder'\n",
    "    files = folder_path.glob('*.txt')          # List all the files in the folder.\n",
    "\n",
    "    # Analyze each file in the folder.\n",
    "    for file_path in files:           \n",
    "\n",
    "        results, words_in_file = analyze_a_file(file_path)    # NEW: Notice 2 values are returned.\n",
    "        \n",
    "        list_of_results.append(results)        # Add the results to the list of results\n",
    "        \n",
    "        list_of_words.extend(words_in_file)    # Add 'words_in_file' to 'list_of_words' \n",
    "        \n",
    "        file_count += 1\n",
    "\n",
    "    # Analyze the group\n",
    "    num_words = count_words(list_of_words)\n",
    "    ttr = get_ttr(list_of_words)\n",
    "    gawl = get_average_word_length(list_of_words)\n",
    "    gfw = get_frequent_words(list_of_words)\n",
    "    big = get_bigrams(list_of_words)\n",
    "    trig = get_trigrams(list_of_words)\n",
    "    pos = get_pos(list_of_words)\n",
    "    \n",
    "    corpus_results = dict()\n",
    "    corpus_results['folder'] = folder\n",
    "    corpus_results['num_words'] = num_words/file_count\n",
    "    corpus_results['ttr'] = ttr\n",
    "    corpus_results['gawl'] = gawl\n",
    "    corpus_results['gfw'] = gfw\n",
    "    corpus_results['big'] = big\n",
    "    corpus_results['trig'] = trig\n",
    "    corpus_results['pos'] = pos\n",
    "    \n",
    "    # Save the list of results to a CSV file\n",
    "    save_results(list_of_results, corpus_results, csv_fpath)  \n",
    "    print(f\"The results are stored in {csv_fpath}\")\n",
    "\n",
    "# Test\n",
    "analyze_a_corpus(\"test/corpus\", \"test/results.csv\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274795f9",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "The following code calls the main function `analyze_a_corpus`, which uses all the other functions defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7edb56bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results are stored in data/Competative FPS.csv\n",
      "The results are stored in data/Non-Competative FPS.csv\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "analyze_a_corpus(\"data/Competative FPS\", \"data/Competative FPS.csv\")\n",
    "analyze_a_corpus(\"data/Non-Competative FPS\", \"data/Non-Competative FPS.csv\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ab11ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9716d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
